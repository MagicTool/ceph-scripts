#!/usr/bin/env bash

echo "This host's filestore osds created by ceph-disk will be erased from the " \
     "cluster. New ones will be made with the bluestore format using ceph-volume"
read -p "Are you sure you want to proceed? (y/n): " -r

case $REPLY in
  y ) echo "Starting osd removal operation.";;
  * ) echo "Exiting..."; exit 0;;
esac

should_exit=false
for package in ceph jq parallel;
do
  if ! rpm -q $package > /dev/null; then
    echo $package " is not installed"
    should_exit=true
  fi
done
if [[ $should_exit == true ]]; then exit 1; fi

set -x

yes 'will cite' | parallel --bibtex > /dev/null 2>&1

if [[ ! -z $1 ]];
then
  osd_file=$1
else
  osd_file=`mktemp --suffix=_osd_list`

  ceph-disk list --format json \
  | jq -r '.[].partitions[]? | select(.journal_dev!=null and .fs_type!="LVM2_member") | [.whoami,.path,.journal_dev] | join("\t")' \
  | sed -r "s/([a-z])[0-9]+/\1/g" > $osd_file
fi

ssds=(`cat $osd_file | awk '{ print $3 }' | sort | uniq`)
ssd_num=${#ssds[*]}

if [[ $ssd_num == 0 ]];
then
  echo "This host does not have any ssds available"
  exit 1
fi

hdd_num=`wc -l < $osd_file`
let "partitions_per_ssd=($hdd_num+$ssd_num-1)/$ssd_num" #ceil(hdd_num/ssd_num)

if [[ $partitions_per_ssd == 1 ]]; then
  echo "Machine has no journals"
  exit 1
fi

let "ssd_partition_size=100/$partitions_per_ssd"

out_and_destroy()
{
  osd_id=$1
  osd_block=$2

  if [[ -z $osd_id  ]]; then
    return 1
  fi

  if [[ `ceph osd metadata $osd_id | jq -r .osd_objectstore` == filestore ]];
  then
    ceph osd out $osd_id
    while ! ceph osd safe-to-destroy $osd_id; do sleep 10 ; done
    systemctl stop ceph-osd@$osd_id

    sleep 5

    if mount | grep -qP "/var/lib/ceph/osd/ceph-$osd_id\b"
    then
      umount /var/lib/ceph/osd/ceph-$osd_id || exit 1
    fi

    ceph osd destroy $osd_id --yes-i-really-mean-it || exit 1
    ceph-volume lvm zap $osd_block || exit 1
  fi
}
export -f out_and_destroy

create_new_osd()
{
  osd_id=$1
  hdd_block=$2
  ssd_block=$4

  if [[ -z $osd_id  ]]; then
    return 1
  fi

  ceph-volume lvm create --bluestore --osd-id $osd_id --data $hdd_block --block.db $ssd_block || exit 1
  ceph osd in $osd_id || exit 1
}
export -f create_new_osd

for ssd in ${ssds[*]};
do
  grep $ssd $osd_file | parallel -C '\t' out_and_destroy

  next=true
  cut -f1 < $osd_file | while read a; do
    if ! ( ceph osd tree | grep -P "osd\.$a\b" | grep destroyed );
    then
      echo "$a is not destroyed with $ssd"
      next=false
    fi
  done
  if [[ $next == false ]]; then continue; fi
  dd if=/dev/zero of=$ssd bs=512 count=1
  parted -s $ssd mklabel gpt
  for i in `seq 0 $(($partitions_per_ssd-1))`;
  do
    parted -s -a optimal $ssd mkpart primary \
                      "$((i*$ssd_partition_size))%" \
                      "$(((i+1)*$ssd_partition_size))%"
  done
  parallel -C '\t' --xapply create_new_osd :::: <(grep $ssd $osd_file) ::: `seq 1 $partitions_per_ssd | sed "s|^|$ssd|"`
done
